{
  "hash": "c6922d1010fe7b2681b05d78f2ba5ae2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression Analysis Example: Optimizing Expected Returns\"\nauthor: \"Josh Tomiyama\"\ndate: \"2025-11-07\"\ncategories:\n  - \"Machine Learning\"\n  - \"Logistic Regression\"\n  - \"Optimization\"\nengine: knitr\nfreeze: auto\ntoc: true\ntoc-depth: 2\ntoc-location: left\nexecute:\n  results: hold\n---\n\nLast Update: January 06, 2026\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(arm)\nlibrary(gtsummary)\nlibrary(dplyr)\nlibrary(ggplot2)\n```\n:::\n\n\n# Introduction\n\nThis will be the one of the first in a series of example analyses I plan to post online to showcase my statistical technical knowledge as well as document some interesting problems I have come across in my experience. I specifically want to focus on what and why I do things over the course of an analysis rather than the how I do the analysis. As such, I will assume some familiarity with the statistical tools and code presented here and generally keep theoretical discussion at a higher level.\n\nThe article is organized into the following parts:\n\n1.  A basic introduction of the logistic regression model and its comparison to other machine learning models\n\n2.  A simulated example detailing the workflow to build a logistic regression model\n\n3.  Methods to leverage the logistic regression model to optimize some target goal\n\n4.  Discussion of how to approach practical problems when the data is less than ideal\n\n# Logistic Regression\n\nLogistic regression is one of the fundamental tools of machine learning and statistics and is actually one of the more powerful tools when one has enough time to go that extra mile and fine tune the model to the data. First I'll just state the mathematical formula for logistic regression beginning with the data model:\n\n$Y_i|\\pi_i \\sim Bernoulli(\\pi_i)$\n\nwhere $i$ refers to the data sample index (think of a row in your data table), $Y_i$ is whether \"success\" is observed for sample $i$, and $\\pi_i$ is the probability of success for the $i$-th subject. In typical applications, $Y_i$ is coded such that 0 means \"failure\" and 1 means \"success\". Now we can move on to the regression part of the model between the log-odds and the covariates of interest:\n\n$\\log(\\frac{\\pi_i}{1-\\pi_i}) = \\pmb x_i \\pmb \\beta$\n\nwhere $\\pmb x_i$ is a vector of covariates that you think explain the probability of success and $\\pmb \\beta$ is the vector of weights that indicate how strongly they contribute to the log-odds of success. When estimating the logistic regression model the $\\pmb \\beta$ coefficients are the main parameters of interest (as $\\pi_i$ is derived from $\\pmb \\beta$). The reason we call it \"logistic\" regression is because $\\log(\\frac{x_i}{1-x_i})$ is the inverse of the logistic function.\n\nThe assumptions of the model are as follows:\n\n1.  The outcome is a binary random variable, specifically a Bernoulli[^1] random variable.\n\n2.  The independent/explanatory variables (I'll commonly refer to these as covariates) have a linear relationship with the log-odds of \"success\". The linearity here refers to the beta parameters, for example one may include an $x^2\\beta$ term in the model as the beta parameter has a power of 1.\n\n3.  Observations are independent of each other\n\n4.  No multicollinearity between covariates. For any single covariate, the other covariates in the model don't perfectly describe that covariate.\n\n5.  No complete separation. In other words, the covariates cannot perfectly predict binary outcome.\n\n[^1]: There is an extension of logistic regression to group level data where the outcome is the number of successes observed and is modeled as a binomial random variable.\n\n## Logistic regression compared to other machine learning models\n\nThe important distinction between logistic regression and other machine learning models is that the statistical theory on the model parameters is well understood. For each of the $\\beta$ parameters in our model, we know the statistical properties to allow inference through p-values or confidence intervals. Logistic regression is still a tool used today to understand, or *infer*, the relationship between a binary outcome and other data collected when statistical evidence is required to validate any findings.\n\nAlthough in most real-world scenarios the other machine learning models perform better in terms of predictiveness, logistic regression can sometimes still be a better model in terms of predictiveness. In my experience through classes and work projects, a logistic regression model is actually superior to the more sophisticated machine learning techniques of Gradient Boosted Trees and Random Forests under a few scenarios with respect to the AUC (area under the curve) performance metric.\n\nThe first scenario I've come across is when the sample size is relatively small. The machine learning techniques tend to work very well when there is more data to help effectively learn the functional relationship between the data you have and the target outcome. I would say this would be the situation when your sample size is in the magnitude of a few hundreds or less. After that point, other machine learning techniques perform at their expected capacity.\n\nThe second scenario where logistic regression can prove better than tree based methods is if one is willing to both select the covariates that strongly relate to the outcome of interest and also identify how each covariate functionally relates to the outcome. To expand on what I mean by \"how each covariate functionally relates to the outcome\", some examples of functional relationships are the usual linear relationship, quadratic relationship, or sinusoidal relationship. In mathematical terms you can propose the following models:\n\n$Linear: \\log(\\frac{\\pi_i}{1 - \\pi_i}) = x_i\\beta$\n\n$Quadratic: \\log(\\frac{\\pi_i}{1 - \\pi_i}) = x_i\\beta_1 + x_i^2\\beta_2$\n\n$Sinusoidal: \\log(\\frac{\\pi_i}{1 - \\pi_i}) = sin(x_i)\\beta$\n\nChoosing a functional relationship alone produces infinite possibilities for one to sort through in addition to just choosing which covariates to include in one's analysis. The strength of tree based approaches is that they learn these relationships with enough data, but of course the tree won't be as accurate as if one were to explicitly identify that \"true\" relationship when training a logistic regression. However, identifying the \"true\" functional relationship between the outcome and covariates is rather impractical when one has many different covariates to consider at once. So once again, when the data becomes sufficiently large, it is more practical to use a Gradient Boosted Tree model or Random Forest model.\n\nThe advantages and shortfalls of logistic regression compared to other machine learning techniques naturally extend to any generalized linear model such as linear regression. To be clear, I am **NOT** saying that in these scenarios generalized linear models will **ALWAYS** surpass the newer machine learning models and algorithms, but rather that these scenarios **ALLOW** generalized linear models to shine just as well, if not better, than their other machine learning counterparts in terms of predictiveness.\n\n# A Novel Application Problem\n\nSuppose that one is starting an online clothing store selling clothes, shoes, and accessories. They have the production cost of each product and wish to know what markup percentage is optimal for generating the most revenue. The data is set up such that we have many offers for products on sale for different markup percentages and it is recorded whether the sale was made or not. From past data, it is plausible that the type of product (clothes, shoes, accessories) has an affect on whether a sale is made.\n\nDefining and estimating the optimal markup percentage is the novel part of the analysis that I think differentiates this analysis from typical logistic regression analyses. If the problem seems somewhat uninspiring, I obfuscated the real application for privacy reasons. This simplified problem does target the core methods and ideas for that application.\n\nBefore jumping into the analysis or looking at the data, it is imperative to decide whether the analysis is attempting to optimize prediction or inference. Although inference and prediction are similar goals and a way Because of the optimization scheme I outline in a later section, I believe having an unbiased estimate of $\\pi_i$ and $\\beta_{markup}$ is the main goal, so I interpret this as predominantly an inference problem because of that.\n\n# Simulating toy data\n\nBefore proceeding with the analysis we shall simulate a toy data set in R that will fit the problem. We will use the following logistic regression model to simulate data:\n\nFrom this set up, we can propose the following logistic regression model:\n\n$$\n\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\beta_0 + x_{markup} * \\beta_{markup} + \nx_{cost} * \\beta_{cost} + x_{type=shoes} * \\beta_{type=shoes} + x_{type = clothes}*\\beta_{type = clothes}\n$$\n\nwhere $x_{markup}$ is the markup percentage above 0 (above 100 is allowed), $x_{cost}$ is cost in dollars, $x_{type=shoes}$ is 1 when product $i$ is shoes and 0 otherwise, and similarly $x_{type = clothes}$ is 1 when product $i$ is clothes and 0 otherwise. When product $i$ is accessories, both $x_{type=shoes}$ and $x_{type = clothes}$ are 0. The way that $x_{type}$ is encoded is known as the dummy variable encoding common in statistics (which is different from the one-hot encoding more prevalent in the machine learning field). The outcome of interest $Y_i$ is a binary variable where 1 indicates the product was sold and 0 the product was not sold, and it follows that $\\pi_i$ is the probability that product $i$ is sold or not. To clarify about the $x_{markup}$ covariate, if $x_{markup} = 20$ then the product was marked up $20\\%$.\n\nI chose the values for true parameters $\\pmb \\beta$ that I felt gave a good spread of the true probabilities while also being representative of the expected behavior in the example problem described to me. I arbitrarily chose a decently large sample size of 3000 to ensure proper estimation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## True parameter values\n\nset.seed(2013)\n\nsample_size <- 3000\n\ntrue_beta <- c(2, # intercept\n               -0.2, # markup\n               0.002, # cost\n               1, # type=clothes\n               1.5 )# type=shoes\n\n### Simulate data\n\ntype <- sample(c(\"accessories\", \"clothes\", \"shoes\"), \n               size = sample_size,\n               replace = TRUE,\n               prob = c(0.3, 0.5, 0.2))\ncost <- rnorm(sample_size, mean = 2000, sd = 400)\nmarkup <- runif(sample_size, min = 20, max = 120)\n\n## make sure markup and cost are above 0\nmarkup <- ifelse(markup <= 0, 10, markup)\ncost <- ifelse(cost <= 0, 2000, cost)\n\n# convert type into a factor variable to prepare for analysis\n\ntype <- factor(type, levels = c(\"accessories\", \"clothes\", \"shoes\"))\n\nx_df <- data.frame(markup = markup, cost = cost, type = type)\n\n# get x_matrix\n\nx_matrix <- model.matrix(~ 1 + markup + cost + type, data = x_df)\n\n#simulated log-odds per person\n\nlog_odds <- x_matrix %*% true_beta\n\n#convert log_odds to probabilities\n\ninverse_logit <- function(x){(1 + exp(-x))^-1}\ntrue_probs <- inverse_logit(log_odds)\n\n# get the outcome\nsold <- rbinom(sample_size, 1, true_probs)\nx_df$sold <- sold\n# x_df$true_probs <- c(true_probs)\n# x_df$true_log_odds <- c(log_odds)\n```\n:::\n\n\nHere are the first few rows of the data set to get an idea of how the data is organized:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(x_df, 5) %>%\n  kable() %>%\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> markup </th>\n   <th style=\"text-align:right;\"> cost </th>\n   <th style=\"text-align:left;\"> type </th>\n   <th style=\"text-align:right;\"> sold </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 106.39550 </td>\n   <td style=\"text-align:right;\"> 2233.739 </td>\n   <td style=\"text-align:left;\"> clothes </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 34.87069 </td>\n   <td style=\"text-align:right;\"> 1786.943 </td>\n   <td style=\"text-align:left;\"> shoes </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 42.20465 </td>\n   <td style=\"text-align:right;\"> 1646.098 </td>\n   <td style=\"text-align:left;\"> accessories </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 21.46863 </td>\n   <td style=\"text-align:right;\"> 1793.615 </td>\n   <td style=\"text-align:left;\"> accessories </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 42.81934 </td>\n   <td style=\"text-align:right;\"> 2247.883 </td>\n   <td style=\"text-align:left;\"> clothes </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n# Example Logistic Regression Analysis\n\n## Quick Exploratory analysis of the Data Set\n\nWhen starting an analysis, it's important to first summarize and get a feel of the data. This is a useful and important step as it informs what are reasonable models for the data and indicates the quality of the data.\n\n\n::: {#tbl-summ .cell layout-nrow=\"2\" tbl-cap='Summary Tables' tbl-subcap='[\"Summary by Type\",\"Summary by Sold\"]'}\n\n```{.r .cell-code}\nx_df %>% \n  tbl_summary(\n    include = c(markup, cost, type, sold),\n    type = list(all_continuous() ~ \"continuous2\"),\n    statistic = list(\n      all_continuous() ~ c(\"{mean} ({sd})\", \"{median} ({p25}, {p75})\", \"[{min}, {max}]\")\n    ),\n    by = type)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"dozzpvrzwh\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#dozzpvrzwh table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#dozzpvrzwh thead, #dozzpvrzwh tbody, #dozzpvrzwh tfoot, #dozzpvrzwh tr, #dozzpvrzwh td, #dozzpvrzwh th {\n  border-style: none;\n}\n\n#dozzpvrzwh p {\n  margin: 0;\n  padding: 0;\n}\n\n#dozzpvrzwh .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#dozzpvrzwh .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#dozzpvrzwh .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#dozzpvrzwh .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#dozzpvrzwh .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#dozzpvrzwh .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#dozzpvrzwh .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#dozzpvrzwh .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#dozzpvrzwh .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#dozzpvrzwh .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#dozzpvrzwh .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#dozzpvrzwh .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#dozzpvrzwh .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#dozzpvrzwh .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#dozzpvrzwh .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dozzpvrzwh .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#dozzpvrzwh .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#dozzpvrzwh .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#dozzpvrzwh .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dozzpvrzwh .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#dozzpvrzwh .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dozzpvrzwh .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#dozzpvrzwh .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dozzpvrzwh .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#dozzpvrzwh .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dozzpvrzwh .gt_left {\n  text-align: left;\n}\n\n#dozzpvrzwh .gt_center {\n  text-align: center;\n}\n\n#dozzpvrzwh .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#dozzpvrzwh .gt_font_normal {\n  font-weight: normal;\n}\n\n#dozzpvrzwh .gt_font_bold {\n  font-weight: bold;\n}\n\n#dozzpvrzwh .gt_font_italic {\n  font-style: italic;\n}\n\n#dozzpvrzwh .gt_super {\n  font-size: 65%;\n}\n\n#dozzpvrzwh .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#dozzpvrzwh .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#dozzpvrzwh .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#dozzpvrzwh .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#dozzpvrzwh .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#dozzpvrzwh .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#dozzpvrzwh .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#dozzpvrzwh .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#dozzpvrzwh div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"label\"><span data-qmd-base64=\"KipDaGFyYWN0ZXJpc3RpYyoq\"><span class='gt_from_md'><strong>Characteristic</strong></span></span></th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"stat_1\"><span data-qmd-base64=\"KiphY2Nlc3NvcmllcyoqICAKTiA9IDg3MQ==\"><span class='gt_from_md'><strong>accessories</strong><br />\nN = 871</span></span><span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;\"><sup>1</sup></span></th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"stat_2\"><span data-qmd-base64=\"KipjbG90aGVzKiogIApOID0gMSw1MjU=\"><span class='gt_from_md'><strong>clothes</strong><br />\nN = 1,525</span></span><span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;\"><sup>1</sup></span></th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"stat_3\"><span data-qmd-base64=\"KipzaG9lcyoqICAKTiA9IDYwNA==\"><span class='gt_from_md'><strong>shoes</strong><br />\nN = 604</span></span><span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;\"><sup>1</sup></span></th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">markup</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_3\" class=\"gt_row gt_center\"><br /></td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Mean (SD)</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">70 (28)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">68 (29)</td>\n<td headers=\"stat_3\" class=\"gt_row gt_center\">69 (29)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Median (Q1, Q3)</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">70 (45, 93)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">67 (43, 92)</td>\n<td headers=\"stat_3\" class=\"gt_row gt_center\">68 (44, 92)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    [Min, Max]</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">[20, 120]</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">[20, 120]</td>\n<td headers=\"stat_3\" class=\"gt_row gt_center\">[20, 120]</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">cost</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_3\" class=\"gt_row gt_center\"><br /></td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Mean (SD)</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">1,990 (386)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">2,022 (404)</td>\n<td headers=\"stat_3\" class=\"gt_row gt_center\">2,014 (404)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Median (Q1, Q3)</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">1,984 (1,731, 2,246)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">2,033 (1,750, 2,297)</td>\n<td headers=\"stat_3\" class=\"gt_row gt_center\">2,006 (1,734, 2,305)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    [Min, Max]</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">[712, 3,263]</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">[533, 3,587]</td>\n<td headers=\"stat_3\" class=\"gt_row gt_center\">[688, 3,153]</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">sold</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">82 (9.4%)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">242 (16%)</td>\n<td headers=\"stat_3\" class=\"gt_row gt_center\">118 (20%)</td></tr>\n  </tbody>\n  <tfoot>\n    <tr class=\"gt_footnotes\">\n      <td class=\"gt_footnote\" colspan=\"4\"><span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;\"><sup>1</sup></span> <span data-qmd-base64=\"biAoJSk=\"><span class='gt_from_md'>n (%)</span></span></td>\n    </tr>\n  </tfoot>\n</table>\n</div>\n```\n\n:::\n\n```{.r .cell-code}\nx_df %>%\n  tbl_summary(\n    include = c(markup, cost, sold),\n    type = list(all_continuous() ~ \"continuous2\"),\n    statistic = list(\n      all_continuous() ~ c(\"{mean} ({sd})\", \"{median} ({p25}, {p75})\", \"[{min}, {max}]\")\n    ),\n    by = sold)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"jieefxmxbc\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#jieefxmxbc table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#jieefxmxbc thead, #jieefxmxbc tbody, #jieefxmxbc tfoot, #jieefxmxbc tr, #jieefxmxbc td, #jieefxmxbc th {\n  border-style: none;\n}\n\n#jieefxmxbc p {\n  margin: 0;\n  padding: 0;\n}\n\n#jieefxmxbc .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#jieefxmxbc .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#jieefxmxbc .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#jieefxmxbc .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#jieefxmxbc .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#jieefxmxbc .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#jieefxmxbc .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#jieefxmxbc .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#jieefxmxbc .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#jieefxmxbc .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#jieefxmxbc .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#jieefxmxbc .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#jieefxmxbc .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#jieefxmxbc .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#jieefxmxbc .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jieefxmxbc .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#jieefxmxbc .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#jieefxmxbc .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#jieefxmxbc .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jieefxmxbc .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#jieefxmxbc .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jieefxmxbc .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#jieefxmxbc .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jieefxmxbc .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#jieefxmxbc .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jieefxmxbc .gt_left {\n  text-align: left;\n}\n\n#jieefxmxbc .gt_center {\n  text-align: center;\n}\n\n#jieefxmxbc .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#jieefxmxbc .gt_font_normal {\n  font-weight: normal;\n}\n\n#jieefxmxbc .gt_font_bold {\n  font-weight: bold;\n}\n\n#jieefxmxbc .gt_font_italic {\n  font-style: italic;\n}\n\n#jieefxmxbc .gt_super {\n  font-size: 65%;\n}\n\n#jieefxmxbc .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#jieefxmxbc .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#jieefxmxbc .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#jieefxmxbc .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#jieefxmxbc .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#jieefxmxbc .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#jieefxmxbc .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#jieefxmxbc .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#jieefxmxbc div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"label\"><span data-qmd-base64=\"KipDaGFyYWN0ZXJpc3RpYyoq\"><span class='gt_from_md'><strong>Characteristic</strong></span></span></th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"stat_1\"><span data-qmd-base64=\"KiowKiogIApOID0gMiw1NTg=\"><span class='gt_from_md'><strong>0</strong><br />\nN = 2,558</span></span></th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"stat_2\"><span data-qmd-base64=\"KioxKiogIApOID0gNDQy\"><span class='gt_from_md'><strong>1</strong><br />\nN = 442</span></span></th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">markup</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\"><br /></td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Mean (SD)</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">75 (26)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">30 (8)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Median (Q1, Q3)</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">76 (54, 96)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">28 (24, 35)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    [Min, Max]</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">[21, 120]</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">[20, 57]</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">cost</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\"><br /></td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\"><br /></td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Mean (SD)</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">1,992 (395)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">2,123 (404)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    Median (Q1, Q3)</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">1,991 (1,728, 2,258)</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">2,129 (1,827, 2,405)</td></tr>\n    <tr><td headers=\"label\" class=\"gt_row gt_left\">    [Min, Max]</td>\n<td headers=\"stat_1\" class=\"gt_row gt_center\">[533, 3,587]</td>\n<td headers=\"stat_2\" class=\"gt_row gt_center\">[1,023, 3,353]</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(markup, cost, \n     main = \"cost vs markup\", \n     ylab = \"cost of production ($)\", \n     xlab = \"markup of product (%)\")\n```\n\n::: {.cell-output-display}\n![Comparing cost and markup](index_files/figure-html/fig-comp-1.png){#fig-comp width=672}\n:::\n:::\n\n\nThe first table summarizes the different covariates and outcome by type. Inspecting the average and quartiles of markup and cost between the different types, we don't notice any difference in the distributions. This implies that type is associated with neither the cost nor markup, which is a best case scenario for a logistic regression analysis. The proportion of successful sales is different among the product types with clothes and shoe products selling at a higher rate than the accessories. This implies that there may be an association between type and product sales. Clearly, there is an imbalance in the distribution of type in the data set, but we have enough cases of each product type such that including type in the logistic regression model shouldn't be a problem.\n\nThe second table summarizes markup and the cost variables by whether the product sold (the 1 group) or not (the 0 group). Comparing these two groups, we can clearly see that markup is lower in the groups that sold and indicates a negative association between sales and markup. The distribution of cost shifts about 100 dollars higher in the group that sold and thus suggests a positive association between cost and sales.\n\nFinally, in the scatter plot comparing cost and markup, we observe random spread of the points. There is no indication of a linear or other functional relationship between markup and cost, which is an ideal scenario for logistic regression.\n\n## Building a logistic regression model\n\nFitting a logistic regression model in R is a simple one line of code. Since we know what the true generating parameters are above, let's double check our work by comparing the fitted values to the true parameters.\n\n\n::: {#tbl-truefit .cell tbl-cap='Comparing estimates to true values. Estimates are on the log-odds scale'}\n\n```{.r .cell-code}\nfit <- glm(sold ~ 1 + markup + cost + type, \n           data = x_df,\n           family = binomial)\nci <- confint(fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n```{.r .cell-code}\nresult <- data.frame(truth = true_beta, \n                     est = coef(fit),\n                     ci_lwr = ci[,1],\n                     ci_upr = ci[,2])\n\nkable(result, digits = 4) %>% \n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> truth </th>\n   <th style=\"text-align:right;\"> est </th>\n   <th style=\"text-align:right;\"> ci_lwr </th>\n   <th style=\"text-align:right;\"> ci_upr </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 2.000 </td>\n   <td style=\"text-align:right;\"> 1.7716 </td>\n   <td style=\"text-align:right;\"> 0.7778 </td>\n   <td style=\"text-align:right;\"> 2.7816 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> markup </td>\n   <td style=\"text-align:right;\"> -0.200 </td>\n   <td style=\"text-align:right;\"> -0.2045 </td>\n   <td style=\"text-align:right;\"> -0.2272 </td>\n   <td style=\"text-align:right;\"> -0.1837 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> cost </td>\n   <td style=\"text-align:right;\"> 0.002 </td>\n   <td style=\"text-align:right;\"> 0.0021 </td>\n   <td style=\"text-align:right;\"> 0.0017 </td>\n   <td style=\"text-align:right;\"> 0.0026 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> typeclothes </td>\n   <td style=\"text-align:right;\"> 1.000 </td>\n   <td style=\"text-align:right;\"> 0.8584 </td>\n   <td style=\"text-align:right;\"> 0.4595 </td>\n   <td style=\"text-align:right;\"> 1.2656 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> typeshoes </td>\n   <td style=\"text-align:right;\"> 1.500 </td>\n   <td style=\"text-align:right;\"> 1.8053 </td>\n   <td style=\"text-align:right;\"> 1.3052 </td>\n   <td style=\"text-align:right;\"> 2.3199 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAs expected with this rather large sample size, the estimates are fairly close to the true values and are well within the 95% confidence intervals. All of the parameter estimates are statistically significant as 0 is not within the confidence intervals, so this is a convincing model. At this step, one would normally interpret these values as odds ratios, but that is not the goal of this analysis, so I will skip the interpretation.\n\nWhen building a logistic regression model, one usually has to consider a few different models to find the best fit to the data. Let's consider three other logistic regression models. One without type in the model but all other variables in the model:\n\n$$\n\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\beta_0 + x_{markup} * \\beta_{markup} + \nx_{cost} * \\beta_{cost}\n$$\n\na second model with a quadratic term for markup and including all the other variables:\n\n$$\n\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\beta_0 + x_{markup} * \\beta_{markup} + x_{markup}^2 * \\beta_{markup^2} +\nx_{cost} * \\beta_{cost} + x_{type=shoes} * \\beta_{type=shoes} + x_{type = clothes}*\\beta_{type = clothes}\n$$\n\nand finally a simple model with just markup:\n\n$$\n\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\beta_0 + x_{markup} * \\beta_{markup}\n$$\n\nThere are many model selection criteria to choose from and which one to use depends on the goal of the analysis. In this application, the goal is estimation and inference, so I would personally advocate for the Bayesian Information Criterion (BIC). The BIC is a quantity that expresses how well the proposed model fits the data while also balancing how many parameters are in a model. More parameters in a model results in a better fit to the data but too many parameters results in overfitting. The problem with an overfit model is the lack of generalizability to new data. Please refer to the BIC() documentation to see the formula for its calculation. BIC favors having less parameters in a model as compared to it's counterpart the Akaike Information Criterion (AIC). Therefore, the BIC is sometimes the preferred model selection criterion when the goal is to identify what covariates to go into a model.\n\nLower values of BIC indicate a better statistical model for the data. A difference of at least 2 is the typical threshold to consider one model superior the other. For differences of less than 2 the models are about equivalent and the statistician must decide which model is superior.\n\n\n::: {#tbl-modselect .cell tbl-cap='Comparing Logistic Regression Models'}\n\n```{.r .cell-code}\nfit1 <- glm(sold ~ 1 + markup + cost, \n           data = x_df,\n           family = binomial)\nfit2 <- glm(sold ~ 1 + markup + I(markup^2) + cost + type, \n           data = x_df,\n           family = binomial)\nfit3 <- glm(sold ~ markup, data = x_df, family = binomial)\n\ndf_model_selection <- data.frame(model = c(\"True_model\", \"no_type\", \"quadratic_markup\", \"Simple_markup\"), \n                                 bic = c(BIC(fit), BIC(fit1), BIC(fit2), BIC(fit3))\n                                 ) \ndf_model_selection %>%    \n  kable(digits = 2, caption = \"Smaller BIC indicates better model\") %>%  \n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Smaller BIC indicates better model</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> model </th>\n   <th style=\"text-align:right;\"> bic </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> True_model </td>\n   <td style=\"text-align:right;\"> 927.72 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> no_type </td>\n   <td style=\"text-align:right;\"> 964.57 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> quadratic_markup </td>\n   <td style=\"text-align:right;\"> 935.31 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Simple_markup </td>\n   <td style=\"text-align:right;\"> 1046.51 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nUpon inspecting the BIC values, the true model is unsurprisingly the lowest BIC and thus the best model. Disregarding our knowledge of the true data generating model, the model chosen at this step strongly translates the trends observed in the exploratory data analysis step, and thus highlights the importance of exploring the data before building models. The simple model with just markup is clearly the worst model with the highest BIC, so although the simple model would drastically simplify our optimal estimation of $\\pi_i$ in the next step, the data indicates a more complex relationship between the outcome and covariates is more appropriate.\n\nAlthough we only inspect 4 models here, usually one would explore many more models and spend the bulk of their time trying to figure out the best model. More complex relationships like interactions and different functional transformations of continuous variables could be considered. Based on the exploratory analysis, this does not seem necessary in this analysis.\n\n## Assessing Model Assumptions\n\nAssessing whether the model assumptions are violated is often overlooked when fitting a logistic regression model. The main ones to assess are linearity of covariates (assumption 2), independent sampling (assumption 3), and multicollinearity (assumption 4). Perfect separation (assumption 5) is normally a warning from R if that occurs.\n\nBased on the exploratory analysis earlier, multicollinearity is not likely not an issue in this data set. We know this for a fact because I generated the data independently. The exploratory analysis revealed that there wasn't any clear relationship between pairs of the covariates, and all of the parameters in the model are statistically significant. These are both great signs that multicollinearity is not present. However, to formally check for multicollinearity, one can check the Variance Inflation Factors (VIFs) in the covariates. I'll skip this step for now but one can read more on VIF theory here: https://online.stat.psu.edu/stat462/node/180/ and how to calculate it using the `car` package in R here: https://www.r-bloggers.com/2023/12/exploring-variance-inflation-factor-vif-in-r-a-practical-guide/.\n\nI will note though that if one adds higher order terms in the regression, for example\n\n$$\n\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\beta_0 + x_{markup} * \\beta_{markup} + x_{markup}^2 * \\beta_{markup^2}\n$$ then instead of the VIF one will need to inspect the Generalized Variance Inflation Factor (GVIF).\n\nThe hardest assumption to verify is the linearity assumption. This only applies to continuous covariates which are markup and cost in this example. We can use the `binnedplot` function in the `arm` package to assess this. What this function does is bin or discretize the continuous variable into equally spaced segments. Within each segment, calculate the residual for the observations via:\n\n$$\n  residual_i = y_i - \\hat \\pi_i \n$$ where $\\hat \\pi_i$ is calculated from a proposed logistic regression model. We inspect the plot to make sure that the residuals are randomly scattered around the $y = 0$ horizontal line, there is no pattern in the residuals, and that at most \\~5% of the residuals are outside of the confidence interval lines.\n\nThese binned residuals plots assume a large sample size for the statistical properties to hold. Specifically **if the sample size is large within each bin**, then the expected value of these residuals is 0 with known standard errors due to the Lindeberg-Feller central limit theorem. My personal rule-of-thumb is at least 30 observations to be reasonably confident in the approximation, but the more the better. Unfortunately, the binned plots from the arm package don't throw warnings when the bins have small sample size, so some caution is needed when interpretting these plots. In addition to large sample sizes, for values of $\\hat \\pi_i$ close to 0 or 1, the statistical theory does not hold, so bins with mostly very low or very high predicted values of $\\hat \\pi_i$ are not as informative.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## example on how the binned residual plot is constructed\nrng <- range(x_df$cost) # min, max of markup\n## break up range into 3 equal parts\nbreakpoints <- seq(from = rng[1],\n                   to = rng[2],\n                   by = diff(rng/3))\nmidpoints <- (breakpoints[-1] + breakpoints[-length(breakpoints)]) / 2\nres <- x_df$sold - predict(fit, type = 'response')\nres_df <- data.frame(res = res,\n           bp = cut(x_df$cost, breaks = breakpoints))\nres_ave <- aggregate(res ~ bp,\n                     data = res_df,\n                     FUN = mean)\nplot(x = x_df$cost, y = res, \n     xlab = \"cost\",\n     ylab = \"Residuals: observed - predicted\",\n     main  = \"How a binned residual plot is constructed\",\n     pch = 16)\npoints(x = midpoints, y = res_ave$res, \n       pch = 16, cex = 1.2,\n       col = 'red')\nabline(v = breakpoints, col = 'red')\nlegend(\"topright\", \n       legend = c(\"observed_residual\",\n                  \"average_residual\"),\n       col = c('black', 'red'),\n       pch = 16)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nLet's look at several binned residual plots to assess whether our proposed model is reasonable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- residuals(fit, type = \"response\")\nbinnedplot(x = fitted(fit),\n           y = res, \n           nclass=NULL, \n           xlab=\"Expected Values\", \n           ylab=\"Average residual\", \n           main=\"Binned residual plot\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbinnedplot(x = x_df$markup,\n           y = res, \n           nclass=NULL, \n           xlab=\"Binned markup\", \n           ylab=\"Average residual\", \n           main=\"Binned residual plot of markup\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nbinnedplot(x = x_df$cost,\n           y = res, \n           nclass=NULL, \n           xlab=\"Binned cost\", \n           ylab=\"Average residual\", \n           main=\"Binned residual plot of cost\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n:::\n\n\n# Optimizing the Expected Returns (Expected Revenue)\n\n## Relationship between $x_{markup}$ and $\\pi_i$\n\nNow that we have decided on a logistic regression model, let's explore the relationship between markup and the probability of selling the product. The relationship between markup and the log-odds might be linear, but when we invert the problem to get the relationship between markup and the probability it is certainly non-linear. To produce this non-linear relationship, we can invert the log-odds back to the probability scale through the following formula:\n\n$$\n\\pi_i = (1 + \\exp[-(\\beta_0 + x_{markup} * \\beta_{markup} + \nx_{cost} * \\beta_{cost} + x_{type=shoes} * \\beta_{type=shoes} + x_{type = clothes}*\\beta_{type = clothes})])^{-1}\n$$\n\nWe are specifically interested in how markup relates to the probability of selling, but we have the other data variables in the formula. Before exploring how the probability of selling changes over a range of values for markup, it is pertinent to choose some representative values for cost and type. To start with, let's set the cost level to be the observed average cost level and let type be the most common type in the data set to provide a curve.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## there isn't a mode function in base R\ngetMode <- function(x){\n  lx <- levels(x)\n  ux <- unique(lx)\n  factor(ux[which.max(tabulate(match(lx, ux)))], levels = lx)\n}\nrange_markup <- seq(from = 0, to = 75, by = 1)\nmode_type <- getMode(x_df$type)\nmean_cost <- mean(x_df$cost)\nnew_x_df <- data.frame(\"markup\" = range_markup,\n                       \"cost\" = mean_cost,\n                       \"type\" = mode_type)\n\npreds <- predict(fit, newdata = new_x_df, se.fit = TRUE)\npreds_ci_upr <- preds$fit + qnorm(0.975)*preds$se.fit\npreds_ci_lwr <- preds$fit - qnorm(0.975)*preds$se.fit\nprobs_df <- data.frame(pred_prob = inverse_logit(preds$fit),\n                       pred_prob_lwr = inverse_logit(preds_ci_lwr),\n                       pred_prob_upr = inverse_logit(preds_ci_upr),\n                       markup = range_markup)\n\ntrue_probs <- model.matrix(~ markup + cost + type, data = new_x_df) %*% true_beta\nprobs_df$true_probs <- inverse_logit(true_probs)\n\n\nggplot(probs_df) + \n  ## probability curve predicted from logistic regression in black\n  geom_line(aes(x = markup, y = pred_prob)) + \n  ## 95% wald CI in red\n  geom_line(aes(x = markup, y = pred_prob_upr), col = \"red\") + \n  geom_line(aes(x = markup, y = pred_prob_lwr), col = \"red\") + \n  ## probability curve using true values will be in blue\n  geom_line(aes(x = markup, y = true_probs), col = \"blue\") + \n  ggtitle(paste0(\"Probablity of Selling Product vs markup percentage\"),\n          subtitle = paste0(\"type = \", mode_type[1], \n                            \"  cost = \", \n                            round(mean_cost[1], digits = 2)\n                            )\n  ) + \n  labs(x = \"markup (%)\", y = \"Probability of Selling Product\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nAs expected, due to how well our estimates of the parameters are, the estimated probability curve in black is mostly overlapped by expected probabilities calculated from the true parameters in blue. As we can see from the graph above, the probability of selling decreases as markup increases. This isn't surprising given that the true parameter for markup is -0.02.\n\nLet's plot the differences between the true and predicted probabilities to really show how small the difference in magnitude is. We can see that the probabilities only differ at most by 0.02. Depending on the application context, this can be a meaningful difference or not, but for this application let's assume that an underestimation of 0.02 is a reasonably close estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x = probs_df$markup, \n     y = probs_df$true_probs - probs_df$pred_prob,\n     type = 'h',\n     ylab = \"True Prob - Pred Prob\",\n     xlab = \"markup\",\n     main = \"Difference between True and Predicted Probabilities\")\n```\n\n::: {.cell-output-display}\n![Differences between predicted probability and true probability](index_files/figure-html/fig-diffprob-1.png){#fig-diffprob width=672}\n:::\n:::\n\n\n## Calculating the Optimal markup\n\nWe can propose the following function to express a balance between optimizing cost markup while increasing the probability of successfully selling the product:\n\n$f(x_{markup}) = (1 + \\frac{x_{markup}}{100})x_{cost} \\pi_i$\n\nThis function is derived from the idea of the expected total revenue generated by a new product. The new products total revenue is expressed as $(1 + \\frac{x_{markup}}{100})*x_{cost}$ and the expected value of a new observation $Y_i$ is $\\pi_i$ because $Y_i$ is a Bernoulli random variable (see appendix for small derivation). Thus, the expected total revenue generated from this product is the multiplication of these two quantities.\n\nThe calculation of $\\pi_i$ depends on $x_{markup}$ and $x_{cost}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpected_return <- function(markup, cost, type, fit, log = FALSE){\n  new_data <- data.frame(markup = markup,\n                         cost = cost,\n                         type = type)\n  # if(markup < 0){\n  #   return(-Inf)\n  # }\n  \n  if(log){\n   ### doing optimization on the log scale is more stable, if markup > 0\n    log(1 + markup/100) + log(cost) + \n      log(predict(fit, newdata = new_data, type = \"response\"))\n  }else{\n   (1 + markup/100)*cost * #total sale price\n    predict(fit, newdata = new_data, type = \"response\") #prob success \n  }\n}\n\noptimal_vals <- optim(par = 50, \n      fn = expected_return, \n      method = \"BFGS\",\n      control = list(fnscale = -1), # do maximization not minimization\n      # These params don't change\n      cost = mean_cost[1], type = mode_type[1], fit = fit \n      )\n\nif(optimal_vals$convergence != 0){\n  stop(\"The numerical optimization did not converge\")\n}else{\n  new_data <- data.frame(markup = optimal_vals$par,\n                         cost = mean_cost[1],\n                         type = mode_type[1])\n  optimal_prob_success <- inverse_logit(predict(fit, new_data))\n  optimal_df <- data.frame(optimal_markup = optimal_vals$par,\n                           expected_total_cost = optimal_vals$value,\n                           type = mode_type[1],\n                           cost = mean_cost[1],\n                           prob_success = optimal_prob_success)\n  kable(optimal_df, digits = 3, align = 'c') %>%\n    kable_styling()\n}\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> optimal_markup </th>\n   <th style=\"text-align:center;\"> expected_total_cost </th>\n   <th style=\"text-align:center;\"> type </th>\n   <th style=\"text-align:center;\"> cost </th>\n   <th style=\"text-align:center;\"> prob_success </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 14.48 </td>\n   <td style=\"text-align:center;\"> 2203.908 </td>\n   <td style=\"text-align:center;\"> accessories </td>\n   <td style=\"text-align:center;\"> 2011.027 </td>\n   <td style=\"text-align:center;\"> 0.957 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nFrom this analysis, we find that for an accessory product that costs of \\$2011.03 to produce, the optimal markup is about 14.48 percent. This results in a total revenue generated of \\$2203.91.\n\nAlthough our formula calculates an optimal markup of 14.48 percent, we need to keep in mind that the range of markup in the data set are values between 20 and 120 percent. Thus, a markup of 14.48 percent is outside the region covered by the data. I would suggest collecting more data in this range before widely adopting this decision rule.\n\nIn this section we only looked at one specific combination of covariates. Of course, we may want to consider cases with combinations of product costs and product types. To explore these different scenarios more easily, I've created a Rshiny dashboard that'll be linked below when it's done.\n\n## Hypothetical Impact of Optimal Markup strategy\n\nLet's assess the business impact of this optimal markup strategy is in terms of two important outcomes:\n\n1.  The change in successful sales compared to the observed sales data\n\n2.  The change in revenue generated compared to the observed revenue\n\nFor each point in our data set, we will substitute the observed optimal markup strategy with the optimal markup and check the change in sales and revenue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalc_optimal_markup <- function(cost, type, fit){\n  optim(par = 50, \n      fn = expected_return, \n      method = \"BFGS\",\n      control = list(fnscale = -1), # do maximization not minimization\n      # These params don't change\n      cost = cost, type = type, fit = fit \n      )  \n}\n\noptimal_markups <- mapply(calc_optimal_markup, \n                          cost = x_df$cost, \n                          type = x_df$type, \n                          MoreArgs = list(fit = fit))\noptimal_x_df <- x_df\noptimal_x_df$markup <- unlist(optimal_markups[1,])\n\nhist(optimal_x_df$markup)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhypothetical_probs <- predict(fit, \n                              newdata = optimal_x_df, \n                              type = 'response')\nest_probs <- predict(fit, newdata = x_df, type = 'response')\n\nset.seed(2013)\nexpected_results <- replicate(5000,\n          expr = {\n            hypothetical_success <- rbinom(sample_size, 1, hypothetical_probs)\n            c(change_sales = sum(hypothetical_success) - sum(x_df$sold),\n              change_rev = sum(hypothetical_success * (1 + optimal_x_df$markup/100)*x_df$cost) - \n                           sum(x_df$sold * (1 + x_df$markup/100)*x_df$cost)\n            )\n          })\nobs_revenue <- sum(x_df$sold * (1 + x_df$markup/100)*x_df$cost)\n\noptimal_results <- data.frame(ave_change_sales = mean(expected_results[1,]),\n                              ave_change_revenue = mean(expected_results[2,]),\n                              ave_percent_change_sales = mean(expected_results[1,])/sum(x_df$sold),\n                              ave_percent_change_revenue = mean(expected_results[2,]) / obs_revenue)\noptimal_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ave_change_sales ave_change_revenue ave_percent_change_sales\n1         2433.894            5664458                 5.506547\n  ave_percent_change_revenue\n1                     4.6332\n```\n\n\n:::\n:::\n\n\n# Discussion\n\nEasy to extend to\n\n# Practical Considerations During Analysis\n\n## Model Misspecification\n\nOne may be tempted to fit a simpler logistic regression model that just includes markup and an intercept term because markup is the only relationship we are interested in exploring. In the optimization step outline earlier, this model simplifies the calculations significantly as we do not need to set fixed values for type and cost. However, if we choose to ignore the \"true\" relationship between the data variables and the outcome then we introduce significant bias in our estimates and predictions. Using the same data from earlier, let's inspect the relationship of the estimated probability of recovery and markup under this simpler model:\n\n$$\n  \\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\beta_0 + x_{markup}*\\beta_{markup}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_reduced <- glm(sold ~ markup, data = x_df, family = binomial()) \n# summary(fit_reduced)  \nnew_x_df <- data.frame(\"markup\" = range_markup,                        \n                       \"cost\" = mean_cost,                        \n                       \"type\" = mode_type)  \n\npreds <- predict(fit_reduced, newdata = new_x_df, se.fit = TRUE) \npreds_ci_upr <- preds$fit + qnorm(0.975)*preds$se.fit \npreds_ci_lwr <- preds$fit - qnorm(0.975)*preds$se.fit \nprobs_df <- data.frame(pred_prob = inverse_logit(preds$fit), \n                       pred_prob_lwr = inverse_logit(preds_ci_lwr), \n                       pred_prob_upr = inverse_logit(preds_ci_upr),  \n                       markup = range_markup)  \ntrue_probs <- model.matrix(~ markup + cost + type, data = new_x_df) %*% true_beta \nprobs_df$true_probs <- inverse_logit(true_probs)  \n# mean(probs_df$pred_prob - probs_df$true_probs)   \nggplot(probs_df) +    \n  geom_line(aes(x = markup, y = pred_prob)) +    \n  geom_line(aes(x = markup, y = pred_prob_upr), col = \"red\") +    \n  geom_line(aes(x = markup, y = pred_prob_lwr), col = \"red\") +    \n  geom_line(aes(x = markup, y = true_probs), col = \"blue\") +    \n  ggtitle(paste0(\"Probablity of sale vs markup\"),\n          subtitle = paste0(\"type = \", mode_type[1],  \n                            \"  cost = \", \n                            round(mean_cost[1], digits = 2)   \n                            )   \n          )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nVery clearly, the true probability curve in blue is outside of the 95% confidence intervals for most of values of markup. The bias is mild at first but quickly diverges after around markup = 20. In this example curve, the estimated probability is higher than the true probability so any decisions made off the estimated probability will be over optimistic and thus lead to waste of markup doses. This clearly highlights the importance of an exploratory analysis in addition to model selection steps to avoid bias in the logistic regression model.\n\n## Influential Data Points and Sample Size\n\nThis question has come to me multiple times where sometimes the data is \"biased\" in some way where some subgroup of the data is underrepresented. The specific case we'll focus on is when some interval of values for a continuous covariate are not observed. For example, suppose that in the data we didn't observe any points that costed between 2000 and 2100. Is this necessarily a problem? Well, it depends on whether these unobserved values follow the same pattern as the rest of the observed data.\n\nFirst, one needs to consider what the application of the study is. If the analysis is for a scientific experiment and the goal is inference, then generally no this violation reveals some error in the experimental setup and thus the inference is hard to justify. If we are not in such a strict scenario, such as the scenario presented in the problem, well then the bias *can* be a problem, but it *depends* on the truth, the quality of the data, and the sample size.\n\nTo simulate this scenario, we will bias the data set from earlier in two (rather extreme) ways:\n\n1.  People with a cost in the range of 2000-2100mg chose not to participate in the study.\n2.  People with markup in the range of 70-80% chose not to participate in the study\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_df_bias <- filter(x_df, cost < 2000 | cost > 2100)\n\nfit_bias <- glm(sold ~ 1 + markup + cost + type, \n                data = x_df_bias,\n                family = binomial)\n\nci_bias <- confint(fit_bias)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n```{.r .cell-code}\nresult <- data.frame(truth = true_beta, \n                     full_est = coef(fit),\n                     bias_est = coef(fit_bias),\n                     bias_ci_lwr = ci_bias[,1],\n                     bias_ci_upr = ci_bias[,2])\nkable(result, digits = 4, caption = \"Estimates when cost is biased\") %>% \n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Estimates when cost is biased</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> truth </th>\n   <th style=\"text-align:right;\"> full_est </th>\n   <th style=\"text-align:right;\"> bias_est </th>\n   <th style=\"text-align:right;\"> bias_ci_lwr </th>\n   <th style=\"text-align:right;\"> bias_ci_upr </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 2.000 </td>\n   <td style=\"text-align:right;\"> 1.7716 </td>\n   <td style=\"text-align:right;\"> 1.7881 </td>\n   <td style=\"text-align:right;\"> 0.7845 </td>\n   <td style=\"text-align:right;\"> 2.8097 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> markup </td>\n   <td style=\"text-align:right;\"> -0.200 </td>\n   <td style=\"text-align:right;\"> -0.2045 </td>\n   <td style=\"text-align:right;\"> -0.2046 </td>\n   <td style=\"text-align:right;\"> -0.2284 </td>\n   <td style=\"text-align:right;\"> -0.1828 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> cost </td>\n   <td style=\"text-align:right;\"> 0.002 </td>\n   <td style=\"text-align:right;\"> 0.0021 </td>\n   <td style=\"text-align:right;\"> 0.0022 </td>\n   <td style=\"text-align:right;\"> 0.0017 </td>\n   <td style=\"text-align:right;\"> 0.0026 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> typeclothes </td>\n   <td style=\"text-align:right;\"> 1.000 </td>\n   <td style=\"text-align:right;\"> 0.8584 </td>\n   <td style=\"text-align:right;\"> 0.8135 </td>\n   <td style=\"text-align:right;\"> 0.3934 </td>\n   <td style=\"text-align:right;\"> 1.2423 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> typeshoes </td>\n   <td style=\"text-align:right;\"> 1.500 </td>\n   <td style=\"text-align:right;\"> 1.8053 </td>\n   <td style=\"text-align:right;\"> 1.7858 </td>\n   <td style=\"text-align:right;\"> 1.2682 </td>\n   <td style=\"text-align:right;\"> 2.3194 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\nx_df_bias2 <- filter(x_df, markup < 70 | markup > 80)\n\nfit_bias2 <- glm(sold ~ 1 + markup + cost + type, \n                data = x_df_bias2,\n                family = binomial)\n\nci_bias <- confint(fit_bias2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n```{.r .cell-code}\nsample_sizes <- data.frame(full = sample_size,\n                           bias1 = nrow(x_df_bias),\n                           bias2 = nrow(x_df_bias2))\n\nresult <- data.frame(truth = true_beta,\n                     full_est = coef(fit),\n                     bias_est = coef(fit_bias2),\n                     bias_ci_lwr = ci_bias[,1],\n                     bias_ci_upr = ci_bias[,2])\nkable(result, digits = 4, caption = \"Estimates when markup is biased\") %>%\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Estimates when markup is biased</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> truth </th>\n   <th style=\"text-align:right;\"> full_est </th>\n   <th style=\"text-align:right;\"> bias_est </th>\n   <th style=\"text-align:right;\"> bias_ci_lwr </th>\n   <th style=\"text-align:right;\"> bias_ci_upr </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 2.000 </td>\n   <td style=\"text-align:right;\"> 1.7716 </td>\n   <td style=\"text-align:right;\"> 1.7590 </td>\n   <td style=\"text-align:right;\"> 0.7637 </td>\n   <td style=\"text-align:right;\"> 2.7703 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> markup </td>\n   <td style=\"text-align:right;\"> -0.200 </td>\n   <td style=\"text-align:right;\"> -0.2045 </td>\n   <td style=\"text-align:right;\"> -0.2040 </td>\n   <td style=\"text-align:right;\"> -0.2267 </td>\n   <td style=\"text-align:right;\"> -0.1830 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> cost </td>\n   <td style=\"text-align:right;\"> 0.002 </td>\n   <td style=\"text-align:right;\"> 0.0021 </td>\n   <td style=\"text-align:right;\"> 0.0021 </td>\n   <td style=\"text-align:right;\"> 0.0017 </td>\n   <td style=\"text-align:right;\"> 0.0026 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> typeclothes </td>\n   <td style=\"text-align:right;\"> 1.000 </td>\n   <td style=\"text-align:right;\"> 0.8584 </td>\n   <td style=\"text-align:right;\"> 0.8577 </td>\n   <td style=\"text-align:right;\"> 0.4590 </td>\n   <td style=\"text-align:right;\"> 1.2646 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> typeshoes </td>\n   <td style=\"text-align:right;\"> 1.500 </td>\n   <td style=\"text-align:right;\"> 1.8053 </td>\n   <td style=\"text-align:right;\"> 1.8034 </td>\n   <td style=\"text-align:right;\"> 1.3035 </td>\n   <td style=\"text-align:right;\"> 2.3177 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nEven though we excluded a specific portion of our target population the estimates shifted but not by a lot. In fact, the inference is still rather okay under these conditions despite the bias (see appendix: simulation study of bias in estimates due to interval of missing values). This is actually not completely unexpected and relates to the concept of influential data points when conducting a data analysis. There are multiple ways to measure influence in data with respect to a generalized linear model, the one most pertinent to this concept is DFBETA(S).\n\nDFBETA(S) measures the change in a parameter estimate when a single data point is deleted from the model. The formula to calculate this measurement is expressed as:\n\n$$DFBETAS_j = \\frac{\\hat \\beta_j - \\hat \\beta_{(i)j}}{SE(\\hat \\beta_j)}$$\n\nWhere $j$ is the index for each beta parameter in the regression model, $\\hat \\beta_{(i)j}$ is the estimated beta parameter when the $i$th data point is deleted, and $SE(\\hat \\beta_j)$ is the standard error for the $\\beta_j$ parameter.\n\nWe can calculate this for each of the data points in our sample and a rule of thumb is that points with a DFBETA(S) greater than $\\pm \\frac{2}{\\sqrt n}$ are highly influential where $n$ is the sample size of the data. The S in DFBETA(S) stands for standardized and is why the denominator is the standard error of $\\hat \\beta_j$. The parentheses in the name are just to emphasize the standardization part, I'll drop them from here on out.\n\nLet's inspect the DFBETAS for the full model to get an idea of how influential our data is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfbetas_vals <- data.frame(dfbetas(fit)) \ncolnames(dfbetas_vals)[1] <- \"intercept\" \nthreshold <- sqrt(4/sample_size) \n\nfor(i in 1:ncol(dfbetas_vals)){ \n  y <- dfbetas_vals[,i] \n  plot(y, \n       type = 'h', \n       main = paste0(\"DFBETAS plot for \", colnames(dfbetas_vals)[i]),\n       ylim = c(min(c(y, -1*threshold)), max(c(y, threshold)))\n       ) \n  abline(h = c(-1,1)*threshold, col = 'red') \n  text(x = 0, \n       max(y), \n       adj = 0, \n       paste0(\"Proportion of highly influential points = \", \n              mean(abs(y) > threshold)) \n       ) \n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-5.png){width=672}\n:::\n\n```{.r .cell-code}\n# idx <- abs(dfbetas_vals$markup) > threshold\n# summary(x_df[idx,])\n# summary(x_df)\n```\n:::\n\n\nHere we can see somewhere around 10% of the points are influential relative to the $\\pm \\frac{2}{\\sqrt n}$ cut off for each of the beta parameters in the logistic regression. In a real world analysis we would inspect these points to determine if there is a pattern emerging from these influential points and appropriately exclude or try to adjust for any implied bias.\n\nAlthough not immediately obvious from the formula for DFBETAS, the sample size impacts how influential a single data point is on the estimation. Generally speaking, the larger the sample size then the lower magnitude of the influence of a single data point on estimating parameters. Thus, for a large sample size like the one in this simulated data set, the influence is small and is the reason why deleting a small portion of the data set had minimal effect on the parameter estimates.\n\nThe lesson here is that, for any \"biased\" data set, the influence may or may not strongly impact the parameters estimates depending on whether the biased data excluded any of these highly influential data points. Because we are in the context of a simulation problem, it's obvious to us how big this influence is because we know the truth. In real applications, we never know how big the influence is; all we know is that a bias in estimates will exist. For instance, if the data was biased in a way that a subset of the target population is excluded, then we will never know how those unobserved data points would impact the parameter estimates. This is where statistical analysis becomes a bit more of an art than a science.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_fit_data <- function(i, bias_data = TRUE, ...){\n  sample_size <- 3000\n\n  true_beta <- c(2, # intercept\n                 -0.2, # markup\n                 0.002, # cost\n                 1, # type=clothes\n                 1.5 )# type=shoes\n  \n  ### Simulate data\n  \n  type <- sample(c(\"accessories\", \"clothes\", \"shoes\"), \n                 size = sample_size,\n                 replace = TRUE,\n                 prob = c(0.3, 0.5, 0.2))\n  cost <- rnorm(sample_size, mean = 2000, sd = 400)\n  markup <- runif(sample_size, min = 20, max = 120)\n  \n  ### make markup and cost are above 0\n  \n  markup <- ifelse(markup <= 0, 10, markup)\n  cost <- ifelse(cost <= 0, 2000, cost)\n  \n  # convert type into a factor variable to prepare for analysis\n  \n  type <- factor(type, levels = c(\"accessories\", \"clothes\", \"shoes\"))\n  \n  x_df <- data.frame(markup = markup, cost = cost, type = type)\n  \n  # get x_matrix\n  \n  x_matrix <- model.matrix(~ 1 + markup + cost + type, data = x_df)\n  \n  #simulated log-odds per person\n  \n  log_odds <- x_matrix %*% true_beta\n  \n  #convert log_odds to probabilities\n  \n  inverse_logit <- function(x){(1 + exp(-x))^-1}\n  true_probs <- inverse_logit(log_odds)\n  \n  # get the outcome\n  sold <- rbinom(sample_size, 1, true_probs)\n  x_df$sold <- sold\n  x_df\n  if(bias_data){\n    x_df_bias <- x_df_bias <- filter(x_df, cost < 2000 | cost > 2100)\n  }else{\n    x_df_bias <- x_df\n  }\n  \n  fit <- glm(sold ~ 1 + markup + cost + type, \n           data = x_df_bias,\n           family = binomial)\n  bias <- coef(fit) - true_beta\n  ci <- confint(fit)\n  ## check whether true parameters are within the confidence interval\n  cover <- (ci[,1] < true_beta) & (true_beta < ci[,2])\n  out <- c(bias, cover)\n  names(out) <- c(names(bias), paste0(\"cover\", names(bias)))\n  out\n}\nset.seed(123123)\nlibrary(parallel)\ncl <- makeCluster(detectCores() - 1)\nclusterEvalQ(cl, suppressPackageStartupMessages(library(dplyr)))\nclusterSetRNGStream(cl)\nnreps <- 10000\nrep_results_biased <- parSapply(cl, 1:nreps, simulate_fit_data, bias_data = TRUE)\nrep_results <- parSapply(cl, 1:nreps, simulate_fit_data, bias_data = FALSE)\nstopCluster(cl)\nrowMeans(rep_results)\nrowMeans(rep_results_biased)\n```\n:::\n\n\n### What to do about percieved bias in a data set\n\nFor each of the bias scenarios, I would first investigate why the sampling bias occurred. Perhaps it was an expected phenomena such as one does not sell products at those cost levels. In this case, we simply redefine the target population of inference to exclude that unobserved sub–population.\n\nIf the sampling bias was not expected in the data and it isn't feasible to collect data from missing sub-population, then what to do next really depends. First I would assess how well the model fits the data and the quality of the data. The data in this example is simulated, so the data quality is perfect (i.e. no correlation between covariates, large enough sample size). An easy next step is to look at some measures of internal model validity like a calibration curve between the predictions and the observed values and cross validation.\n\nTODO: Explanation of calibration curves goes here\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(CalibrationCurves)\nval.prob.ci.2(predict(fit_bias, type = \"response\"), fit_bias$y, main = \"Biased cost sampling\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nval.prob.ci.2(p = predict(fit_bias, type = \"response\"), y = fit_bias$y, \n    main = \"Biased cost sampling\")\n\nA 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic. \n\n          Dxy       C (ROC)            R2             D      D:Chi-sq \n 9.455531e-01  9.727765e-01  7.377389e-01  5.421879e-01  1.481173e+03 \n          D:p             U      U:Chi-sq           U:p             Q \n 0.000000e+00 -7.326007e-04  4.547474e-13  1.000000e+00  5.429205e-01 \n        Brier     Intercept         Slope          Emax  Brier scaled \n 4.639351e-02 -6.178465e-09  1.000000e+00  3.418414e-09  6.320478e-01 \n         Eavg           ECI \n 5.579221e-03  1.200259e-02 \n```\n\n\n:::\n\n```{.r .cell-code}\nval.prob.ci.2(predict(fit_bias2, type = \"response\"), fit_bias2$y, main = \"Biased markup sampling\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nval.prob.ci.2(p = predict(fit_bias2, type = \"response\"), y = fit_bias2$y, \n    main = \"Biased markup sampling\")\n\nA 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic. \n\n          Dxy       C (ROC)            R2             D      D:Chi-sq \n 9.377667e-01  9.688834e-01  7.293933e-01  5.629314e-01  1.517537e+03 \n          D:p             U      U:Chi-sq           U:p             Q \n 0.000000e+00 -7.423905e-04 -3.410605e-13  1.000000e+00  5.636738e-01 \n        Brier     Intercept         Slope          Emax  Brier scaled \n 5.170820e-02 -1.478349e-09  1.000000e+00  8.040748e-10  6.229804e-01 \n         Eavg           ECI \n 4.285675e-03  4.562992e-03 \n```\n\n\n:::\n:::\n\n\n# Appendix: Deriving expected revenue formula\n\nFirst, we calculate that $(1 + \\frac{x_{markup}}{100})x_{cost}$ would be the sales price of the item (ignoring any taxes).\n\nNext let's derive a simple random variable $Z_i$ to represent the revenue generated from each sale. Consider the random variable $Z_i$ which is a transformation of our previous outcome variable $Y_i$:\n\n$$Z_i = (1 + \\frac{x_{markup}}{100})x_{cost} Y_i$$\n\nA straightforward application of deriving shows that\n\n$$\\mathbb E(Z_i) = (1 + \\frac{x_{markup}}{100})x_{cost} \\mathbb E(Y_i) =  (1 + \\frac{x_{markup}}{100})x_{cost} \\pi_i$$\n\nWe have a (maximum-likelihood) estimate of $\\pi_i$ that we will denote as $\\hat \\pi_i$. Thus our (maximum-likelihood) estimate of $E(Z_i)$ is\n\n$\\widehat{\\mathbb E(Z_i)} = (1 + \\frac{x_{markup}}{100})x_{cost} \\hat \\pi_i$.\n\nFor the more statistically rigorous, the expectations here are conditional on the covariates in the logistic regression model and $\\widehat{\\mathbb E(Z_i)}$ is a maximum-likelihood estimator of $\\mathbb E(Z_i)$.\n\n# Session Info and Bibliography\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.2 (2025-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=en_US.UTF-8                  \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: Pacific/Honolulu\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] CalibrationCurves_3.0.0 rms_8.1-0               Hmisc_5.2-4            \n [4] ggplot2_4.0.1           dplyr_1.1.4             gtsummary_2.5.0        \n [7] arm_1.14-4              lme4_1.1-38             Matrix_1.7-4           \n[10] MASS_7.3-65             kableExtra_1.4.0        knitr_1.51             \n\nloaded via a namespace (and not attached):\n  [1] mathjaxr_2.0-0            RColorBrewer_1.1-3       \n  [3] rstudioapi_0.17.1         jsonlite_2.0.0           \n  [5] shape_1.4.6.1             magrittr_2.0.4           \n  [7] TH.data_1.1-5             farver_2.1.2             \n  [9] nloptr_2.2.1              rmarkdown_2.30           \n [11] fs_1.6.6                  vctrs_0.6.5              \n [13] minqa_1.2.8               CompQuadForm_1.4.4       \n [15] base64enc_0.1-3           htmltools_0.5.9          \n [17] forcats_1.0.1             polspline_1.1.25         \n [19] broom_1.0.11              Formula_1.2-5            \n [21] sass_0.4.10               parallelly_1.46.0        \n [23] htmlwidgets_1.6.4         sandwich_3.1-1           \n [25] zoo_1.8-15                gt_1.2.0                 \n [27] commonmark_2.0.0          mime_0.13                \n [29] lifecycle_1.0.4           cmprsk_2.2-12            \n [31] iterators_1.0.14          pkgconfig_2.0.3          \n [33] R6_2.6.1                  fastmap_1.2.0            \n [35] rbibutils_2.4             future_1.68.0            \n [37] shiny_1.12.1              digest_0.6.39            \n [39] numDeriv_2016.8-1.1       colorspace_2.1-2         \n [41] furrr_0.3.1               textshaping_1.0.4        \n [43] labeling_0.4.3            metadat_1.4-0            \n [45] abind_1.4-8               riskRegression_2025.09.17\n [47] compiler_4.5.2            withr_3.0.2              \n [49] htmlTable_2.4.3           S7_0.2.1                 \n [51] backports_1.5.0           metafor_4.8-0            \n [53] meta_8.2-1                broom.mixed_0.2.9.6      \n [55] lava_1.8.2                quantreg_6.1             \n [57] tools_4.5.2               foreign_0.8-90           \n [59] otel_0.2.0                httpuv_1.6.16            \n [61] future.apply_1.20.1       nnet_7.3-20              \n [63] glue_1.8.0                promises_1.5.0           \n [65] mets_1.3.8                nlme_3.1-168             \n [67] grid_4.5.2                checkmate_2.3.3          \n [69] cluster_2.1.8.1           generics_0.1.4           \n [71] gtable_0.3.6              tzdb_0.5.0               \n [73] tidyr_1.3.2               data.table_1.18.0        \n [75] hms_1.1.4                 xml2_1.5.1               \n [77] foreach_1.5.2             pillar_1.11.1            \n [79] markdown_2.0              stringr_1.6.0            \n [81] merTools_0.6.3            later_1.4.4              \n [83] splines_4.5.2             lattice_0.22-7           \n [85] survival_3.8-3            SparseM_1.84-2           \n [87] tidyselect_1.2.1          blme_1.0-6               \n [89] reformulas_0.4.3          gridExtra_2.3            \n [91] bookdown_0.46             litedown_0.9             \n [93] svglite_2.2.2             xfun_0.55                \n [95] stringi_1.8.7             yaml_2.3.12              \n [97] boot_1.3-32               pec_2025.06.24           \n [99] evaluate_1.0.5            codetools_0.2-20         \n[101] tibble_3.3.0              cli_3.6.5                \n[103] rpart_4.1.24              xtable_1.8-4             \n[105] systemfonts_1.3.1         Rdpack_2.6.4             \n[107] Rcpp_1.1.0                globals_0.18.0           \n[109] coda_0.19-4.1             parallel_4.5.2           \n[111] MatrixModels_0.5-4        readr_2.1.6              \n[113] timeROC_0.4               listenv_0.10.0           \n[115] glmnet_4.1-10             viridisLite_0.4.2        \n[117] mvtnorm_1.3-3             timereg_2.0.7            \n[119] scales_1.4.0              prodlim_2025.04.28       \n[121] purrr_1.2.0               rlang_1.1.6              \n[123] multcomp_1.4-29           cards_0.7.1              \n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}